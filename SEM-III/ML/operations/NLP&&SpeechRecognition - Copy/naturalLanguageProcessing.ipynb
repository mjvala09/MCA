{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing\n",
    "# --------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Are you curious about tokenization? Let's see how it works! We need to analyze a couple of sentences with punctuations to see it in action.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are you curious about tokenization?', \"Let's see how it works!\", 'We need to analyze a couple of sentences with punctuations to see it in action.']\n"
     ]
    }
   ],
   "source": [
    "sent_tokenize_list = sent_tokenize(text)\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'s\", 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokenize_list = word_tokenize(text)\n",
    "print(word_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Are', 'you', 'curious', 'about', 'tokenization', '?', 'Let', \"'\", 's', 'see', 'how', 'it', 'works', '!', 'We', 'need', 'to', 'analyze', 'a', 'couple', 'of', 'sentences', 'with', 'punctuations', 'to', 'see', 'it', 'in', 'action', '.']\n"
     ]
    }
   ],
   "source": [
    "word_punkt_tokenizer = WordPunctTokenizer()\n",
    "print(word_punkt_tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table ['tabl', 'tabl', 'tabl']\n",
      "probably ['prob', 'probabl', 'probabl']\n",
      "wolves ['wolv', 'wolv', 'wolv']\n",
      "playing ['play', 'play', 'play']\n",
      "is ['is', 'is', 'is']\n",
      "cats ['cat', 'cat', 'cat']\n",
      "the ['the', 'the', 'the']\n",
      "beaches ['beach', 'beach', 'beach']\n",
      "grounded ['ground', 'ground', 'ground']\n",
      "Commonly ['common', 'commonli', 'common']\n",
      "envision ['envid', 'envis', 'envis']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "words=['table','probably','wolves','playing','is','cats','the','beaches','grounded','Commonly','envision']\n",
    "stemmers=['PORTER','LANCASTER','SNOWBALL']\n",
    "\n",
    "lancasterStemmer = LancasterStemmer()\n",
    "porterStemmer = PorterStemmer()\n",
    "snowballStemmer = SnowballStemmer('english')\n",
    "\n",
    "for word in words:\n",
    "    stemmedWord = [lancasterStemmer.stem(word), porterStemmer.stem(word), snowballStemmer.stem(word)]\n",
    "    print(word,stemmedWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table       \t--> table table\n",
      "probably       \t--> probably probably\n",
      "wolves       \t--> wolves wolf\n",
      "playing       \t--> play playing\n",
      "is       \t--> be is\n",
      "cats       \t--> cat cat\n",
      "the       \t--> the the\n",
      "beaches       \t--> beach beach\n",
      "grounded       \t--> ground grounded\n",
      "Commonly       \t--> Commonly Commonly\n",
      "envision       \t--> envision envision\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    " \n",
    "for word in words:\n",
    "    lemmatizedWord = [wnl.lemmatize(word, pos='v'), wnl.lemmatize(word, pos='n')]\n",
    "    print(word,\"      \\t-->\", *lemmatizedWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "\n",
    "def splitter(data, n):\n",
    "    words = data.split(\" \")\n",
    "    output = []\n",
    "    c_count = 0\n",
    "    c_words = []\n",
    "    for word in words:\n",
    "        c_words.append(word)\n",
    "        c_count += 1\n",
    "        if(c_count == n):\n",
    "            output.append(\" \".join(c_words))\n",
    "            c_count = 0\n",
    "            c_words = []\n",
    "    output.append(\" \".join(c_words))\n",
    "    return(output)\n",
    "\n",
    "data = \" \".join(brown.words()[:10000])\n",
    "# print(data)\n",
    "n = 1700\n",
    "output = splitter(data, n)\n",
    "print(len(output))\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE WILL COVER IT AFTER DEATH\n",
    "\n",
    "\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# categoryMap = {'misc.forsale':'Sales', 'rec.motorcycles' : 'Motorcycles', 'rec.sport.baseball' : 'Baseball', 'sci.crypt':'Cryptography', 'sci.space' : 'Space'}\n",
    "# trainingData = fetch_20newsgroups(subset='train', categories=categoryMap.keys(), shuffle=True, random_state=7)\n",
    "# print(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aaron', 'male'), ('Abbey', 'male')]\n",
      "84.81404958677686\n",
      "Leonardo --> female\n",
      "Amy --> female\n",
      "Sem --> female\n",
      "levely --> female\n",
      "king --> female\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "def gender_features(word,n=2):\n",
    "    return ({'feature': word[-n:].lower()})\n",
    "labels=[(name,'male') for name in names.words('male.txt')]+[(name,'female') for name in names.words('female.txt')]\n",
    "print(labels[1:3])\n",
    "#random.seed(7)\n",
    "#random.shuffle(labels)\n",
    "data=['Leonardo','Amy','Sem','levely','king']\n",
    "\n",
    "\n",
    "feature_set=[(gender_features(n,3),gender) for (n,gender) in labels]\n",
    "training,testing=feature_set[500:],feature_set[200:]\n",
    "classifier=NaiveBayesClassifier.train(training)\n",
    "print(accuracy(classifier,testing)*100)\n",
    "for name in data:\n",
    "    print(name,'-->',classifier.classify(gender_features(name,5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Aaron', 'male'), ('Abbey', 'male')]\n",
      "63.09400826446281\n",
      "Leonardo      \t-->  female\n",
      "Amy      \t-->  female\n",
      "Sem      \t-->  female\n",
      "levely      \t-->  female\n",
      "king      \t-->  female\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "def genderFeature(names, n=2):\n",
    "    return ({'feature' : word[-1:].lower()})\n",
    "\n",
    "labels = [(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]\n",
    "print(labels[1:3])\n",
    "data=['Leonardo','Amy','Sem','levely','king']\n",
    "feature_set = [(genderFeature(n, 3),gender) for (n, gender) in labels]\n",
    "training, testing = feature_set[500:], feature_set[200:]\n",
    "classifier = NaiveBayesClassifier.train(training)\n",
    "print(accuracy(classifier, testing)*100)\n",
    "\n",
    "for name in data:\n",
    "    print(name, \"     \\t--> \", classifier.classify(genderFeature(name,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 1\n",
      "76.2\n",
      "Leonardo     \t-->  male\n",
      "Amy     \t-->  female\n",
      "Sem     \t-->  male\n",
      "levely     \t-->  female\n",
      "king     \t-->  male\n",
      "\n",
      "\n",
      " 2\n",
      "78.60000000000001\n",
      "Leonardo     \t-->  male\n",
      "Amy     \t-->  female\n",
      "Sem     \t-->  male\n",
      "levely     \t-->  female\n",
      "king     \t-->  male\n",
      "\n",
      "\n",
      " 3\n",
      "76.6\n",
      "Leonardo     \t-->  male\n",
      "Amy     \t-->  female\n",
      "Sem     \t-->  female\n",
      "levely     \t-->  female\n",
      "king     \t-->  male\n",
      "\n",
      "\n",
      " 4\n",
      "70.8\n",
      "Leonardo     \t-->  male\n",
      "Amy     \t-->  female\n",
      "Sem     \t-->  female\n",
      "levely     \t-->  female\n",
      "king     \t-->  male\n"
     ]
    }
   ],
   "source": [
    "# Second using random function\n",
    "import random\n",
    "from nltk.corpus import names\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "def genderFeature(word, n=2):\n",
    "    return ({'feature': word[-n:].lower()})\n",
    "\n",
    "labels = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
    "random.seed(7)\n",
    "random.shuffle(labels)\n",
    "data=['Leonardo','Amy','Sem','levely','king']\n",
    "\n",
    "for i in range(1,5):\n",
    "    print(\"\\n\\n\",i)\n",
    "    feature_set = [(genderFeature(n,i), gender) for (n, gender) in labels]\n",
    "    training, testing = feature_set[500:], feature_set[:500]\n",
    "    classifier = NaiveBayesClassifier.train(training)\n",
    "    print(accuracy(classifier, testing)*100)\n",
    "    for name in data:\n",
    "        print(name, \"    \\t--> \", classifier.classify(genderFeature(name, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive || Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy if a classifier :  0.735\n",
      "================================================================================\n",
      "Top 10 Most Informative Words:\n",
      "outstanding\n",
      "insulting\n",
      "vulnerable\n",
      "ludicrous\n",
      "uninvolving\n",
      "astounding\n",
      "avoids\n",
      "fascination\n",
      "affecting\n",
      "animators\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'color'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\ML\\operations\\NLP&&SpeechRecognition\\naturalLanguageProcessing.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ML/operations/NLP%26%26SpeechRecognition/naturalLanguageProcessing.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m prob_list\u001b[39m=\u001b[39mclassifier\u001b[39m.\u001b[39mprob_classify(extract_features(i\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/ML/operations/NLP%26%26SpeechRecognition/naturalLanguageProcessing.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m pred_sentiment\u001b[39m=\u001b[39mprob_list\u001b[39m.\u001b[39mmax()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ML/operations/NLP%26%26SpeechRecognition/naturalLanguageProcessing.ipynb#X31sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mprint\u001b[39m(i,\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m=>\u001b[39m\u001b[39m\"\u001b[39m,pred_sentiment\u001b[39m.\u001b[39;49mcolor(\u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'color'"
     ]
    }
   ],
   "source": [
    "import nltk.classify.util \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "def extract_features(wordlist):\n",
    "    return dict([(word,True) for word in wordlist])\n",
    "\n",
    "#retrieving positive and negative fileids from nltk.movie_reviews\n",
    "p_fields=movie_reviews.fileids('pos')\n",
    "n_fields=movie_reviews.fileids('neg')\n",
    "\n",
    "#saperating fields into positive and negative reviews\n",
    "f_positive=[(extract_features(movie_reviews.words(fileids=[f])),\"Positive\") for f in p_fields]\n",
    "f_negative=[(extract_features(movie_reviews.words(fileids=[f])),\"Negative\") for f in n_fields]\n",
    "\n",
    "#dividing data into training and testing datasets\n",
    "factor=0.8\n",
    "fPositive=int(factor*len(f_positive))\n",
    "fNegative=int(factor*len(f_negative))\n",
    "\n",
    "#extract the features\n",
    "f_train=f_positive[:fPositive] + f_negative[:fNegative]\n",
    "f_test=f_positive[fPositive:] + f_negative[fNegative:]\n",
    "\n",
    "#using naivebase classifier for classification\n",
    "classifier=NaiveBayesClassifier.train(f_train)\n",
    "print(\"Accuracy if a classifier : \",end=\" \")\n",
    "print(nltk.classify.util.accuracy(classifier,f_test))\n",
    "print(\"=\"*80)\n",
    "\n",
    "#extracting most informative words  from classifier\n",
    "print(\"Top 10 Most Informative Words:\")\n",
    "for i in classifier.most_informative_features()[:10]:\n",
    "    print(i[0])\n",
    "print(\"=\"*80)\n",
    "    \n",
    "data=['it is an amazing movie','this is a dull movie. i will never recommend it to anyone.',\n",
    "      'the cinemetography is pretty great in this movie.',\n",
    "      'the direction was terrible and the story was all over the place.','SAHO is the best movie.',\n",
    "     'She is bad girl.']\n",
    "for i in data:\n",
    "    prob_list=classifier.prob_classify(extract_features(i.split(\" \")))\n",
    "    pred_sentiment=prob_list.max()\n",
    "    print(i,\" \\t=>\",pred_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
